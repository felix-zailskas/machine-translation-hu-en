{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from preprocessing import apply_default_pipeline, split_and_save_dataframe, trim_outliers, add_sentence_tokens\n",
    "from constants import *\n",
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/sampled_data.csv\", header=None, names=[\"en\", \"hu\"])\n",
    "\n",
    "df = apply_default_pipeline(df, \"en\", \"en_processed\", lang=\"en\")\n",
    "df = apply_default_pipeline(df, \"hu\", \"hu_processed\", lang=\"hu\")\n",
    "\n",
    "df[\"en_tokens\"] = df[\"en_processed\"].apply(wordpunct_tokenize)\n",
    "df[\"hu_tokens\"] = df[\"hu_processed\"].apply(wordpunct_tokenize)\n",
    "df = trim_outliers(df, \"en_tokens\", MAX_WORDS)\n",
    "df = trim_outliers(df, \"hu_tokens\", MAX_WORDS)\n",
    "df = df.drop(\"en_tokens\", axis=1)\n",
    "df = df.drop(\"hu_tokens\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_and_save_dataframe(\n",
    "    df, 0.7, 0.1, 0.2, \"preprocessed\", \"../data/preprocessed/\", random_state=0\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 ('NLP_Proj2-rNUYsnK4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5f409a3a832281d5d29b9db2f1b88490e5520e0b507b00f44db5cda2918a49b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
